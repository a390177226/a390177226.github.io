<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python on ZZB's blog</title><link>https://a390177226.github.io/categories/python/</link><description>Recent content in python on ZZB's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>zzb2021.</copyright><lastBuildDate>Thu, 24 Jun 2021 16:12:14 +0800</lastBuildDate><atom:link href="https://a390177226.github.io/categories/python/index.xml" rel="self" type="application/rss+xml"/><item><title>python爬虫[5]--Scrapy框架</title><link>https://a390177226.github.io/python/scrapy/</link><pubDate>Thu, 24 Jun 2021 16:12:14 +0800</pubDate><guid>https://a390177226.github.io/python/scrapy/</guid><description>简介 Scrapy是一个专门用于异步爬虫的框架
功能：高性能的数据解析、请求发送，持久化存储，全站数据爬取，中间件，分布式
安装：pip install scrapy</description></item><item><title>python爬虫[4]--cookie+代理操作+验证码识别+模拟登录</title><link>https://a390177226.github.io/python/cookie/</link><pubDate>Thu, 24 Jun 2021 13:09:39 +0800</pubDate><guid>https://a390177226.github.io/python/cookie/</guid><description>主题 cookie 代理机制 验证码识别 模拟登录 cookie 简介 cookie是存储在客户端的一组键值对
web中cookie的典型应用：免密登录
cookie和爬虫之间的关联
有时，对一张页面进行请求的时候，如果请求的过程不携带cookie的话，则无法请求到正确的页面数据 因此，cookie是爬虫中一个非常典型且常见的反爬机制 案例 需求：爬取雪球网中的咨询信息
url：https://xueqiu.com/ 分析 判定爬取的咨询数据是否为动态加载的 相关的更多咨询数据是动态加载的，滚轮滑动到底部的时候会动态加载出更多咨询数据 定位到ajax请求的数据包，提取出请求的url，响应数据为json形式的咨询数据 # 失败案例 import requests headers = { &amp;#39;user-agent&amp;#39;:&amp;#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36 Edg/91.0.864.48&amp;#39; } url = &amp;#39;https://xueqiu.com/statuses/hot/listV2.json?since_id=-1&amp;amp;max_id=219925&amp;amp;size=15&amp;#39; page_text= requests.get(url=url, headers=headers).json() 问题：我们没有请求到想要的数据
原因：没有严格意义上模拟浏览器发请求
处理：可以将浏览器发请求携带的请求头，全部粘贴在headers字典中，将headers作用到requests的请求操作中即可，优先考虑cookie
cookie的处理方式：
方式1：手动处理 将抓包工具中的cookie粘贴在headers中 弊端：cookie如果过了有效时长则该方式失效 方式2：自动处理 基于Session对象实现自动处理 如何获取一个Session对象：requests.</description></item><item><title>python爬虫[3]--数据解析</title><link>https://a390177226.github.io/python/dataparse/</link><pubDate>Fri, 18 Jun 2021 15:07:15 +0800</pubDate><guid>https://a390177226.github.io/python/dataparse/</guid><description>方法 正则表达式 bs4 xpath（重点） 原理 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储 进行指定标签的定位 标签或者标签的对应属性中存储的数据值进行提取（解析） 爬取一张图片 url = &amp;#39;https://pic.qiushibaike.com/system/pictures/12443/124437393/medium/6O8M8JJRDUFEGBAF.jpg&amp;#39; # content返回二进制形式的图片数据 # text（字符串） content（二进制） json() （对象） img_data = requests.get(url=url).content # 保存至本地 with open(&amp;#39;./test.jpg&amp;#39;,&amp;#39;wb&amp;#39;) as fp: fp.write(img_data) 正则案例 爬取糗事百科中热图板块第一页的所有图片 url = &amp;#39;https://www.qiushibaike.com/imgrank/&amp;#39; # 先爬取url对应的一整张页面 page = requests.get(url=url,headers=headers) page.encoding = &amp;#39;utf-8&amp;#39; page_text = page.text 使用开发者工具查看一张图片的html源码为
&amp;lt;div class=&amp;#34;thumb&amp;#34;&amp;gt; &amp;lt;a href=&amp;#34;/article/124437391&amp;#34; target=&amp;#34;_blank&amp;#34;&amp;gt; &amp;lt;img src=&amp;#34;//pic.qiushibaike.com/system/pictures/12443/124437391/medium/ZS4RPHR1F7GTTTGC.jpg&amp;#34; alt=&amp;#34;糗事#124437391&amp;#34; class=&amp;#34;illustration&amp;#34; width=&amp;#34;100%&amp;#34; height=&amp;#34;auto&amp;#34;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; 提取图片的src的正则表达式：
ex = '&amp;lt;div class=&amp;quot;thumb&amp;quot;&amp;gt;.</description></item><item><title>python爬虫[2]--requests模块</title><link>https://a390177226.github.io/python/requests/</link><pubDate>Thu, 10 Jun 2021 15:07:15 +0800</pubDate><guid>https://a390177226.github.io/python/requests/</guid><description>网络请求模块 urllib模块：古老繁琐 requests模块：简洁高效 requests模块 python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。
作用：模拟浏览器发送请求。
安装：pip install requests
topics 数据解析 动态数据的爬取 selenium 移动端数据的爬取 异步的爬虫 10种反爬机制 第一个例子 爬取搜狗首页的页面数据
import requests # step1: 指定url url = &amp;#39;https://www.sogou.com/&amp;#39; # step2：发起请求 # get方法会返回一个响应对象 response = requests.get(url=url) # step3: 获取响应数据 # .text返回的是字符串形式的响应数据 page_text = response.text # step4: 持久化存储 with open(&amp;#39;./sogou.html&amp;#39;,&amp;#39;w&amp;#39;,encoding=&amp;#39;utf-8&amp;#39;) as fp: fp.write(page_text) 简易网页采集器 基于搜狗针对指定不同的关键字将其对应的页面数据进行爬取
keyWord = input(&amp;#39;enter a key word:&amp;#39;) # 携带了请求参数的url，如果想要爬取不同关键字对应的页面，我们需要将url携带的参数进行动态化 # 实现参数动态化： params = { &amp;#39;query&amp;#39;:keyWord } url = &amp;#39;https://www.</description></item><item><title>python爬虫[1]--概述</title><link>https://a390177226.github.io/python/webscraper_overview/</link><pubDate>Tue, 01 Jun 2021 18:28:53 +0800</pubDate><guid>https://a390177226.github.io/python/webscraper_overview/</guid><description>Links 视频教程
7天学会Python爬虫-轻松爬取各种网站数据实战(2021完整版)| b站 18天学会Python爬虫基础到进阶（爬虫逆向到起飞）| b站 路飞学城-学习文档 (apeland.cn) 概述 简单来说，网络爬虫就是自动从互联网中定向或不定向地采集信息的一种程序。
应用：搜索引擎、采集数据
爬虫的分类 通用爬虫：通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。 简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理(提取关键字、去掉广告)，最后提供一个用户检索接口。 聚焦爬虫：聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。 增量式爬虫：增量式是用来检测网站数据更新的情况，且可以将网站更新的数据进行爬取（后期会有章节单独对其展开详细的讲解）。 数据爬取的流程 指定url 发起请求 获取响应数据 数据解析 持久化存储 scrapy框架 异步的爬虫框架</description></item><item><title>Python基础</title><link>https://a390177226.github.io/python/basic/</link><pubDate>Mon, 31 May 2021 18:43:42 +0800</pubDate><guid>https://a390177226.github.io/python/basic/</guid><description>简介 Python是一门面向对象的、解释型的编程语言。
优点：简洁、易学、几乎全能、支持面向对象。
应用：数据分析与挖掘、黑客逆向工程、网络爬虫、机器学习、开发Web项目、开发游戏、自动化运维
if语句 计算身体质量指数BMI
体质指数（BMI）= 1e4 * 体重（kg）÷ 身高²（cm）
分类 BMI范围 偏瘦 &amp;lt;= 18.4 正常 18.5 ~ 23.9 过重 24.0 ~ 27.9 肥胖 &amp;gt;= 28.0 height = eval(input(&amp;#34;请输入身高(cm)：&amp;#34;)) weight = eval(input(&amp;#34;请输入体重(kg)：&amp;#34;)) BMI = 1e4 * weight / height**2 if BMI &amp;lt; 18.5: category = &amp;#34;偏瘦&amp;#34; elif BMI &amp;lt; 24.0: category = &amp;#34;正常&amp;#34; elif BMI &amp;lt; 28.</description></item></channel></rss>